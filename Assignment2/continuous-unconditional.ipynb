{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef2597af",
   "metadata": {},
   "source": [
    "Waveform generation conditional on mel spectrogram\n",
    "the whole pipeline for baseline models are in this file\n",
    "I also pasted some of the diffwave model code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c9d593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import soundfile as sf\n",
    "\n",
    "# Dataset definition sampling a piece of Mel and corresponding ground truth waveform from the dataset\n",
    "class Mel2WaveDataset(Dataset):\n",
    "    def __init__(self, data_dir, sr=22050, hop_length=256, crop_mel_frames=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.sr = sr\n",
    "        self.hop_length = hop_length\n",
    "        self.crop_mel_frames = crop_mel_frames\n",
    "        wav_paths = glob.glob(os.path.join(data_dir, \"*.wav\"))\n",
    "        self.basenames = [os.path.splitext(os.path.basename(p))[0] for p in wav_paths]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.basenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name = self.basenames[idx]\n",
    "        mel_full = np.load(os.path.join(self.data_dir, f\"{name}.wav.spec.npy\"))\n",
    "        mel_full = torch.from_numpy(mel_full).float()\n",
    "        n_mels, T_mel_total = mel_full.shape\n",
    "\n",
    "        wav_full, sr_load = sf.read(os.path.join(self.data_dir, f\"{name}.wav\"))\n",
    "        wav_full = torch.from_numpy(wav_full).float()\n",
    "        T_wav_full = wav_full.size(0)\n",
    "\n",
    "        if self.crop_mel_frames is not None and T_mel_total >= 1:\n",
    "            K = self.crop_mel_frames\n",
    "            if T_mel_total >= K:\n",
    "                start_frame = random.randint(0, T_mel_total - K)\n",
    "                mel = mel_full[:, start_frame : start_frame + K]\n",
    "                start_sample = start_frame * self.hop_length\n",
    "                wav_len = K * self.hop_length\n",
    "                end_sample = start_sample + wav_len\n",
    "                if end_sample <= T_wav_full:\n",
    "                    wav = wav_full[start_sample : end_sample]\n",
    "                else:\n",
    "                    valid = max(0, T_wav_full - start_sample)\n",
    "                    part = wav_full[start_sample:] if valid > 0 else torch.zeros(0)\n",
    "                    pad_len = wav_len - valid\n",
    "                    wav = torch.cat([part, torch.zeros(pad_len)], dim=0)\n",
    "            else:\n",
    "                pad_mel = torch.zeros(n_mels, K - T_mel_total)\n",
    "                mel = torch.cat([mel_full, pad_mel], dim=1)\n",
    "                wav_needed = K * self.hop_length\n",
    "                if T_wav_full >= wav_needed:\n",
    "                    wav = wav_full[:wav_needed]\n",
    "                else:\n",
    "                    pad_wav = torch.zeros(wav_needed - T_wav_full)\n",
    "                    wav = torch.cat([wav_full, pad_wav], dim=0)\n",
    "        else:\n",
    "            mel = mel_full\n",
    "            wav = wav_full\n",
    "\n",
    "\n",
    "        if torch.mean(torch.abs(wav)) < 1e-3:\n",
    "            new_idx = random.randint(0, len(self.basenames) - 1)\n",
    "            return self.__getitem__(new_idx)\n",
    "\n",
    "        return mel, wav\n",
    "\n",
    "# A simple MLP model\n",
    "class Mel2WaveMLP(nn.Module):\n",
    "    def __init__(self, n_mels=80, K=64, hop_length=256, hidden_dim=1024):\n",
    "        super().__init__()\n",
    "        self.n_mels = n_mels\n",
    "        self.K = K\n",
    "        self.hop_length = hop_length\n",
    "        self.wav_len = K * hop_length\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_mels * K, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, self.wav_len),\n",
    "        )\n",
    "\n",
    "    def forward(self, mel):\n",
    "        B, n_mels, K = mel.shape\n",
    "        x = mel.reshape(B, -1)\n",
    "        wav_pred = self.net(x)\n",
    "        return wav_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c6afd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training pipeline\n",
    "data_dir = \"train_diffwave_aug\"\n",
    "crop_mel_frames = 64\n",
    "n_mels = 80\n",
    "hop_length = 256\n",
    "batch_size = 8\n",
    "lr = 3e-5                      \n",
    "total_epochs = 20\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "dataset = Mel2WaveDataset(\n",
    "    data_dir=data_dir,\n",
    "    sr=22050,\n",
    "    hop_length=hop_length,\n",
    "    crop_mel_frames=crop_mel_frames\n",
    ")\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "\n",
    "mlp_model = Mel2WaveMLP(n_mels=n_mels, K=crop_mel_frames, hop_length=hop_length, hidden_dim=1024).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(mlp_model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "for epoch in range(1, total_epochs):\n",
    "    mlp_model.train()\n",
    "    run_loss = 0.0\n",
    "    for mel, wav_gt in dataloader:\n",
    "        mel = mel.to(device)\n",
    "        wav_gt = wav_gt.to(device)\n",
    "\n",
    "        wav_pred = mlp_model(mel)\n",
    "        wav_pred = torch.clamp(wav_pred, -1.0, 1.0)\n",
    "        wav_gt   = torch.clamp(wav_gt,   -1.0, 1.0)\n",
    "\n",
    "        loss = torch.nn.functional.mse_loss(wav_pred, wav_gt)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(mlp_model.parameters(), max_norm=1.0) \n",
    "        optimizer.step()\n",
    "\n",
    "        run_loss += loss.item() * mel.size(0)\n",
    "    avg = run_loss / len(dataset)\n",
    "    print(f\"[Warm-up {epoch:02d}/{total_epochs:02d}] MSE Loss: {avg:.6f}\")\n",
    "\n",
    "\n",
    "torch.save(mlp_model.state_dict(), \"mel2wave_mlp_spectral_stable.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ff6883",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mel2WaveConvNet(nn.Module):\n",
    "    def __init__(self, n_mels=80, K=64, hop_length=256):\n",
    "        super().__init__()\n",
    "        self.n_mels = n_mels\n",
    "        self.K = K\n",
    "        self.hop_length = hop_length\n",
    "        self.upsample_factor = 4 \n",
    "        self.num_upsamples = 4   \n",
    "        self.final_wav_len = K * (hop_length)  \n",
    "\n",
    "        #project n_mel to hidden channels#\n",
    "        self.conv_in = nn.Conv1d(in_channels=n_mels,\n",
    "                                 out_channels=512,\n",
    "                                 kernel_size=3,\n",
    "                                 padding=1)         \n",
    "        self.bn_in = nn.BatchNorm1d(512)\n",
    "\n",
    "        #4 upsample layers that expand in time dimension and shrink in hidden channels#\n",
    "        self.deconv1 = nn.ConvTranspose1d(in_channels=512,\n",
    "                                          out_channels=256,\n",
    "                                          kernel_size=4,\n",
    "                                          stride=4,\n",
    "                                          padding=0)     \n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "\n",
    "        self.deconv2 = nn.ConvTranspose1d(in_channels=256,\n",
    "                                          out_channels=128,\n",
    "                                          kernel_size=4,\n",
    "                                          stride=4,\n",
    "                                          padding=0)      \n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "\n",
    "        self.deconv3 = nn.ConvTranspose1d(in_channels=128,\n",
    "                                          out_channels=64,\n",
    "                                          kernel_size=4,\n",
    "                                          stride=4,\n",
    "                                          padding=0)      \n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.deconv4 = nn.ConvTranspose1d(in_channels=64,\n",
    "                                          out_channels=32,\n",
    "                                          kernel_size=4,\n",
    "                                          stride=4,\n",
    "                                          padding=0)       \n",
    "        self.bn4 = nn.BatchNorm1d(32)\n",
    "\n",
    "        #further shrink the number of channels to the mono track waveform#\n",
    "        self.conv_mid = nn.Conv1d(in_channels=32,\n",
    "                                  out_channels=16,\n",
    "                                  kernel_size=3,\n",
    "                                  padding=1)        \n",
    "        self.bn_mid = nn.BatchNorm1d(16)\n",
    "\n",
    "        self.conv_out = nn.Conv1d(in_channels=16,\n",
    "                                  out_channels=1,\n",
    "                                  kernel_size=7,\n",
    "                                  padding=3)      \n",
    "\n",
    "        self.act_out = nn.Tanh()\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, mel):\n",
    "\n",
    "        x = self.conv_in(mel)            # → (batch, 512, K)\n",
    "        x = self.relu(self.bn_in(x))\n",
    "\n",
    "        x = self.deconv1(x)              # → (batch, 256, 4K)\n",
    "        x = self.relu(self.bn1(x))\n",
    "\n",
    "        x = self.deconv2(x)              # → (batch, 128, 16K)\n",
    "        x = self.relu(self.bn2(x))\n",
    "\n",
    "        x = self.deconv3(x)              # → (batch,  64, 64K)\n",
    "        x = self.relu(self.bn3(x))\n",
    "\n",
    "        x = self.deconv4(x)              # → (batch,  32, 256K) or (batch, 32, K*hop_length)\n",
    "        x = self.relu(self.bn4(x))\n",
    "\n",
    "        x = self.conv_mid(x)             # → (batch, 16, 256K)\n",
    "        x = self.relu(self.bn_mid(x))\n",
    "\n",
    "        x = self.conv_out(x)             # → (batch, 1, 256K)\n",
    "        wav = self.act_out(x)            # → (batch, 1, 256K)\n",
    "\n",
    "        return wav.squeeze(1)            # → (batch, 256K)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a3338c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training pipline\n",
    "data_dir = \"train_diffwave\"\n",
    "crop_mel_frames = 64    # K\n",
    "n_mels = 80\n",
    "hop_length = 256\n",
    "batch_size = 8\n",
    "lr = 1e-4\n",
    "epochs = 50\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Dataset/ DataLoader\n",
    "dataset = Mel2WaveDataset(data_dir=data_dir,\n",
    "                          sr=22050,\n",
    "                          hop_length=hop_length,\n",
    "                          crop_mel_frames=crop_mel_frames)\n",
    "dataloader = DataLoader(dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True,\n",
    "                        num_workers=4,\n",
    "                        drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b4c479",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Mel2WaveConvNet(n_mels=n_mels,\n",
    "                        K=crop_mel_frames,\n",
    "                        hop_length=hop_length).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e76cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for mel, wav_gt in dataloader:\n",
    "\n",
    "        mel = mel.to(device)\n",
    "        wav_gt = wav_gt.to(device)\n",
    "\n",
    "\n",
    "        wav_pred = model(mel)  \n",
    "\n",
    "        loss = criterion(wav_pred, wav_gt)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * mel.size(0)\n",
    "\n",
    "    avg_loss = epoch_loss / len(dataset)\n",
    "    print(f\"Epoch {epoch:02d}/{epochs:02d}, Loss: {avg_loss:.6f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"mel2wave_convnet.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827e1e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inference block for baseline models\n",
    "import math\n",
    "\n",
    "mlp_model = Mel2WaveMLP(n_mels=80, K=64, hop_length=256, hidden_dim=1024).to(device)\n",
    "mlp_model.load_state_dict(torch.load(\"mel2wave_mlp_spectral_stable.pth\", map_location=device))\n",
    "mlp_model.eval()\n",
    "\n",
    "conv_model = Mel2WaveConvNet(n_mels=80, K=64, hop_length=256).to(device)\n",
    "conv_model.load_state_dict(torch.load(\"mel2wave_convnet.pth\", map_location=device))\n",
    "conv_model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_mels = 80\n",
    "K = 64\n",
    "hop_length = 256\n",
    "sr = 22050\n",
    "\n",
    "def infer_mlp_arbitrary_length(\n",
    "    mlp_model,\n",
    "    mel_npy_path,\n",
    "    output_wav_path,\n",
    "    n_mels=80,\n",
    "    K=64,\n",
    "    hop_length=256,\n",
    "    sr=22050,\n",
    "    device=torch.device(\"cpu\"),\n",
    "):\n",
    "\n",
    "    mel_np = np.load(mel_npy_path)  # shape (n_mels, T_mel_total)\n",
    "    if mel_np.ndim != 2 or mel_np.shape[0] != n_mels:\n",
    "        raise ValueError(f\"mel.npy should be ({n_mels}, T), but got {mel_np.shape}\")\n",
    "\n",
    "    T_mel_total = mel_np.shape[1]\n",
    "    num_chunks = math.ceil(T_mel_total / K)\n",
    "    wav_segments = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_chunks):\n",
    "            start = i * K\n",
    "            end = start + K\n",
    "            if end <= T_mel_total:\n",
    "                mel_seg = mel_np[:, start:end]  # (n_mels, K)\n",
    "            else:\n",
    "                valid = mel_np[:, start:T_mel_total]\n",
    "                pad_len = K - (T_mel_total - start)\n",
    "                mel_seg = np.concatenate([valid, np.zeros((n_mels, pad_len), dtype=mel_np.dtype)], axis=1)\n",
    "\n",
    "            mel_tensor = torch.from_numpy(mel_seg).float().unsqueeze(0).to(device)  # (1, n_mels, K)\n",
    "            wav_pred = mlp_model(mel_tensor)  # (1, K*hop_length)\n",
    "            wav_pred = wav_pred.squeeze(0).cpu().numpy()  # (K*hop_length,)\n",
    "\n",
    "            if end > T_mel_total:\n",
    "                valid_frames = T_mel_total - start\n",
    "                valid_samples = valid_frames * hop_length\n",
    "                wav_pred = wav_pred[:valid_samples]\n",
    "\n",
    "            wav_segments.append(wav_pred)\n",
    "\n",
    "    wav_full = np.concatenate(wav_segments, axis=0)\n",
    "    os.makedirs(os.path.dirname(output_wav_path), exist_ok=True)\n",
    "    sf.write(output_wav_path, wav_full, sr)\n",
    "    print(f\"generated {output_wav_path} with {wav_full.shape[0]} points\")\n",
    "\n",
    "\n",
    "def infer_conv_arbitrary_length(\n",
    "    conv_model,\n",
    "    mel_npy_path,\n",
    "    output_wav_path,\n",
    "    n_mels=80,\n",
    "    K=64,\n",
    "    hop_length=256,\n",
    "    sr=22050,\n",
    "    device=torch.device(\"cpu\"),\n",
    "):\n",
    "    mel_np = np.load(mel_npy_path)  # shape (n_mels, T_mel_total)\n",
    "    if mel_np.ndim != 2 or mel_np.shape[0] != n_mels:\n",
    "        raise ValueError(f\"mel.npy should be ({n_mels}, T), but got {mel_np.shape}\")\n",
    "\n",
    "    T_mel_total = mel_np.shape[1]\n",
    "    num_chunks = math.ceil(T_mel_total / K)\n",
    "    wav_segments = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_chunks):\n",
    "            start = i * K\n",
    "            end = start + K\n",
    "            if end <= T_mel_total:\n",
    "                mel_seg = mel_np[:, start:end]\n",
    "            else:\n",
    "                valid = mel_np[:, start:T_mel_total]\n",
    "                pad_len = K - (T_mel_total - start)\n",
    "                mel_seg = np.concatenate([valid, np.zeros((n_mels, pad_len), dtype=mel_np.dtype)], axis=1)\n",
    "\n",
    "            mel_tensor = torch.from_numpy(mel_seg).float().unsqueeze(0).to(device)  # (1, n_mels, K)\n",
    "            wav_pred = conv_model(mel_tensor)  # (1, K*hop_length)\n",
    "            wav_pred = wav_pred.squeeze(0).cpu().numpy()  # (K*hop_length,)\n",
    "\n",
    "            if end > T_mel_total:\n",
    "                valid_frames = T_mel_total - start\n",
    "                valid_samples = valid_frames * hop_length\n",
    "                wav_pred = wav_pred[:valid_samples]\n",
    "\n",
    "            wav_segments.append(wav_pred)\n",
    "\n",
    "    wav_full = np.concatenate(wav_segments, axis=0)\n",
    "    os.makedirs(os.path.dirname(output_wav_path), exist_ok=True)\n",
    "    sf.write(output_wav_path, wav_full, sr)\n",
    "    print(f\"generated{output_wav_path} {wav_full.shape[0]} points\")\n",
    "\n",
    "\n",
    "infer_mlp_arbitrary_length(\n",
    "    mlp_model=mlp_model,\n",
    "    mel_npy_path=\"yequ.npy\",\n",
    "    output_wav_path=\"outputs/mlp_output.wav\",\n",
    "    n_mels=n_mels,\n",
    "    K=K,\n",
    "    hop_length=hop_length,\n",
    "    sr=sr,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "infer_conv_arbitrary_length(\n",
    "    conv_model=conv_model,\n",
    "    mel_npy_path=\"yequ.npy\",\n",
    "    output_wav_path=\"outputs/conv_output.wav\",\n",
    "    n_mels=n_mels,\n",
    "    K=K,\n",
    "    hop_length=hop_length,\n",
    "    sr=sr,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05361cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation block\n",
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import math\n",
    "\n",
    "\n",
    "def load_wav(path, sr=None):\n",
    "    wav, orig_sr = sf.read(path)\n",
    "    wav = wav.astype(np.float32)\n",
    "    if sr is not None and orig_sr != sr:\n",
    "        wav = librosa.resample(wav, orig_sr, sr)\n",
    "    return wav\n",
    "\n",
    "\n",
    "def pad_or_trim(a, b):\n",
    "    min_len = min(len(a), len(b))\n",
    "    return a[:min_len], b[:min_len]\n",
    "\n",
    "\n",
    "def compute_time_domain_metrics(wav_ref, wav_gen):\n",
    "    ref, gen = pad_or_trim(wav_ref, wav_gen)\n",
    "\n",
    "    diff = ref - gen\n",
    "    mse = np.mean(diff ** 2)\n",
    "    mae = np.mean(np.abs(diff))\n",
    "\n",
    "    eps = 1e-8\n",
    "    signal_power = np.sum(ref ** 2) + eps\n",
    "    noise_power = np.sum(diff ** 2) + eps\n",
    "    snr = 10 * math.log10(signal_power / noise_power)\n",
    "\n",
    "    return {'mse': mse, 'mae': mae, 'snr_db': snr}\n",
    "\n",
    "\n",
    "def compute_spectral_metrics(wav_ref, wav_gen, sr, n_fft=1024, hop_length=256):\n",
    "    ref, gen = pad_or_trim(wav_ref, wav_gen)\n",
    "\n",
    "    S_ref = np.abs(librosa.stft(ref, n_fft=n_fft, hop_length=hop_length))\n",
    "    S_gen = np.abs(librosa.stft(gen, n_fft=n_fft, hop_length=hop_length))\n",
    "\n",
    "    min_freq_bins = min(S_ref.shape[0], S_gen.shape[0])\n",
    "    min_time_steps = min(S_ref.shape[1], S_gen.shape[1])\n",
    "    S_ref = S_ref[:min_freq_bins, :min_time_steps]\n",
    "    S_gen = S_gen[:min_freq_bins, :min_time_steps]\n",
    "\n",
    "    diff = S_ref - S_gen\n",
    "    sc_num = np.linalg.norm(diff, ord='fro')\n",
    "    sc_den = np.linalg.norm(S_ref, ord='fro') + 1e-8\n",
    "    spectral_convergence = sc_num / sc_den\n",
    "\n",
    "\n",
    "    log_ref = np.log(S_ref + 1e-8)\n",
    "    log_gen = np.log(S_gen + 1e-8)\n",
    "    lsd_frame = np.linalg.norm(log_ref - log_gen, axis=0)  \n",
    "    log_stft_distance = np.mean(lsd_frame)\n",
    "\n",
    "    return {\n",
    "        'spectral_convergence': spectral_convergence,\n",
    "        'log_stft_distance': log_stft_distance\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_mcd(wav_ref, wav_gen, sr, n_mels=80, n_mfcc=13, hop_length=256):\n",
    "    ref, gen = pad_or_trim(wav_ref, wav_gen)\n",
    "\n",
    "    mfcc_ref = librosa.feature.mfcc(\n",
    "        y=ref, sr=sr, n_mfcc=n_mfcc, n_mels=n_mels, hop_length=hop_length\n",
    "    )  # shape=(n_mfcc, T_ref)\n",
    "    mfcc_gen = librosa.feature.mfcc(\n",
    "        y=gen, sr=sr, n_mfcc=n_mfcc, n_mels=n_mels, hop_length=hop_length\n",
    "    )  # shape=(n_mfcc, T_gen)\n",
    "\n",
    "    min_frames = min(mfcc_ref.shape[1], mfcc_gen.shape[1])\n",
    "    mfcc_ref = mfcc_ref[:, :min_frames]\n",
    "    mfcc_gen = mfcc_gen[:, :min_frames]\n",
    "\n",
    "    diff = mfcc_ref - mfcc_gen  # shape=(n_mfcc, min_frames)\n",
    "    dist_per_frame = np.linalg.norm(diff, axis=0)  # (min_frames,)\n",
    "    mcd = (10.0 / math.log(10.0)) * math.sqrt(2.0) * np.mean(dist_per_frame + 1e-8)\n",
    "    return mcd\n",
    "\n",
    "\n",
    "def evaluate_wav_pair(\n",
    "    wav_ref_path,\n",
    "    wav_gen_path,\n",
    "    sr=22050,\n",
    "    n_fft=1024,\n",
    "    hop_length=256,\n",
    "    n_mels=80,\n",
    "    n_mfcc=13\n",
    "):\n",
    "\n",
    "    wav_ref = load_wav(wav_ref_path, sr=sr)\n",
    "    wav_gen = load_wav(wav_gen_path, sr=sr)\n",
    "\n",
    "    td_metrics = compute_time_domain_metrics(wav_ref, wav_gen)\n",
    "\n",
    "\n",
    "    spec_metrics = compute_spectral_metrics(wav_ref, wav_gen, sr, n_fft=n_fft, hop_length=hop_length)\n",
    "\n",
    "\n",
    "    mcd_value = compute_mcd(wav_ref, wav_gen, sr, n_mels=n_mels, n_mfcc=n_mfcc, hop_length=hop_length)\n",
    "\n",
    "    results = {\n",
    "        **td_metrics,\n",
    "        **spec_metrics,\n",
    "        'mcd': mcd_value\n",
    "    }\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "wav_ref_path = \"yequ.wav\"\n",
    "\n",
    "wav_gen_path = \"output/mlp_output.wav\"\n",
    "\n",
    "metrics = evaluate_wav_pair(\n",
    "    wav_ref_path=wav_ref_path,\n",
    "    wav_gen_path=wav_gen_path,\n",
    "    sr=22050,\n",
    "    n_fft=1024,\n",
    "    hop_length=256,\n",
    "    n_mels=80,\n",
    "    n_mfcc=13\n",
    ")\n",
    "\n",
    "print(\"===== result =====\")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"{k}: {v:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ee0d52",
   "metadata": {},
   "source": [
    "convolution \n",
    "===== result =====\n",
    "mse: 0.093086\n",
    "mae: 0.242965\n",
    "snr_db: -0.018113\n",
    "spectral_convergence: 0.995097\n",
    "log_stft_distance: 64.287231\n",
    "mcd: 1572.881348\n",
    "\n",
    "mlp\n",
    "===== result =====\n",
    "mse: 0.092785\n",
    "mae: 0.242588\n",
    "snr_db: -0.004060\n",
    "spectral_convergence: 0.991616\n",
    "log_stft_distance: 48.453880\n",
    "mcd: 1165.654419\n",
    "\n",
    "diffwave_pretrained\n",
    "===== result =====\n",
    "mse: 0.109099\n",
    "mae: 0.261110\n",
    "snr_db: -0.707505\n",
    "spectral_convergence: 0.781583\n",
    "log_stft_distance: 28.555260\n",
    "mcd: 375.424744\n",
    "\n",
    "diffwave_finetuned\n",
    "===== result =====\n",
    "mse: 0.151037\n",
    "mae: 0.301934\n",
    "snr_db: -2.120124\n",
    "spectral_convergence: 0.402407\n",
    "log_stft_distance: 25.203579\n",
    "mcd: 217.240768\n",
    "\n",
    "diffwave_finetuned\n",
    "===== result =====\n",
    "mse: 0.160350\n",
    "mae: 0.310657\n",
    "snr_db: -2.379991\n",
    "spectral_convergence: 0.352338\n",
    "log_stft_distance: 24.570955\n",
    "mcd: 170.337021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cc3312",
   "metadata": {},
   "outputs": [],
   "source": [
    "#diffwave model body\n",
    "# Copyright 2020 LMNT, Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "Linear = nn.Linear\n",
    "ConvTranspose2d = nn.ConvTranspose2d\n",
    "\n",
    "\n",
    "def Conv1d(*args, **kwargs):\n",
    "  layer = nn.Conv1d(*args, **kwargs)\n",
    "  nn.init.kaiming_normal_(layer.weight)\n",
    "  return layer\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def silu(x):\n",
    "  return x * torch.sigmoid(x)\n",
    "\n",
    "\n",
    "class DiffusionEmbedding(nn.Module):\n",
    "  def __init__(self, max_steps):\n",
    "    super().__init__()\n",
    "    self.register_buffer('embedding', self._build_embedding(max_steps), persistent=False)\n",
    "    self.projection1 = Linear(128, 512)\n",
    "    self.projection2 = Linear(512, 512)\n",
    "\n",
    "  def forward(self, diffusion_step):\n",
    "    if diffusion_step.dtype in [torch.int32, torch.int64]:\n",
    "      x = self.embedding[diffusion_step]\n",
    "    else:\n",
    "      x = self._lerp_embedding(diffusion_step)\n",
    "    x = self.projection1(x)\n",
    "    x = silu(x)\n",
    "    x = self.projection2(x)\n",
    "    x = silu(x)\n",
    "    return x\n",
    "\n",
    "  def _lerp_embedding(self, t):\n",
    "    low_idx = torch.floor(t).long()\n",
    "    high_idx = torch.ceil(t).long()\n",
    "    low = self.embedding[low_idx]\n",
    "    high = self.embedding[high_idx]\n",
    "    return low + (high - low) * (t - low_idx)\n",
    "\n",
    "  def _build_embedding(self, max_steps):\n",
    "    steps = torch.arange(max_steps).unsqueeze(1)  # [T,1]\n",
    "    dims = torch.arange(64).unsqueeze(0)          # [1,64]\n",
    "    table = steps * 10.0**(dims * 4.0 / 63.0)     # [T,64]\n",
    "    table = torch.cat([torch.sin(table), torch.cos(table)], dim=1)\n",
    "    return table\n",
    "\n",
    "\n",
    "class SpectrogramUpsampler(nn.Module):\n",
    "  def __init__(self, n_mels):\n",
    "    super().__init__()\n",
    "    self.conv1 = ConvTranspose2d(1, 1, [3, 32], stride=[1, 16], padding=[1, 8])\n",
    "    self.conv2 = ConvTranspose2d(1, 1,  [3, 32], stride=[1, 16], padding=[1, 8])\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = torch.unsqueeze(x, 1)\n",
    "    x = self.conv1(x)\n",
    "    x = F.leaky_relu(x, 0.4)\n",
    "    x = self.conv2(x)\n",
    "    x = F.leaky_relu(x, 0.4)\n",
    "    x = torch.squeeze(x, 1)\n",
    "    return x\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "  def __init__(self, n_mels, residual_channels, dilation, uncond=False):\n",
    "    '''\n",
    "    :param n_mels: inplanes of conv1x1 for spectrogram conditional\n",
    "    :param residual_channels: audio conv\n",
    "    :param dilation: audio conv dilation\n",
    "    :param uncond: disable spectrogram conditional\n",
    "    '''\n",
    "    super().__init__()\n",
    "    self.dilated_conv = Conv1d(residual_channels, 2 * residual_channels, 3, padding=dilation, dilation=dilation)\n",
    "    self.diffusion_projection = Linear(512, residual_channels)\n",
    "    if not uncond: # conditional model\n",
    "      self.conditioner_projection = Conv1d(n_mels, 2 * residual_channels, 1)\n",
    "    else: # unconditional model\n",
    "      self.conditioner_projection = None\n",
    "\n",
    "    self.output_projection = Conv1d(residual_channels, 2 * residual_channels, 1)\n",
    "\n",
    "  def forward(self, x, diffusion_step, conditioner=None):\n",
    "    assert (conditioner is None and self.conditioner_projection is None) or \\\n",
    "           (conditioner is not None and self.conditioner_projection is not None)\n",
    "\n",
    "    diffusion_step = self.diffusion_projection(diffusion_step).unsqueeze(-1)\n",
    "    y = x + diffusion_step\n",
    "    if self.conditioner_projection is None: # using a unconditional model\n",
    "      y = self.dilated_conv(y)\n",
    "    else:\n",
    "      conditioner = self.conditioner_projection(conditioner)\n",
    "      y = self.dilated_conv(y) + conditioner\n",
    "\n",
    "    gate, filter = torch.chunk(y, 2, dim=1)\n",
    "    y = torch.sigmoid(gate) * torch.tanh(filter)\n",
    "\n",
    "    y = self.output_projection(y)\n",
    "    residual, skip = torch.chunk(y, 2, dim=1)\n",
    "    return (x + residual) / sqrt(2.0), skip\n",
    "\n",
    "\n",
    "class DiffWave(nn.Module):\n",
    "  def __init__(self, params):\n",
    "    super().__init__()\n",
    "    self.params = params\n",
    "    self.input_projection = Conv1d(1, params.residual_channels, 1)\n",
    "    self.diffusion_embedding = DiffusionEmbedding(len(params.noise_schedule))\n",
    "    if self.params.unconditional: # use unconditional model\n",
    "      self.spectrogram_upsampler = None\n",
    "    else:\n",
    "      self.spectrogram_upsampler = SpectrogramUpsampler(params.n_mels)\n",
    "\n",
    "    self.residual_layers = nn.ModuleList([\n",
    "        ResidualBlock(params.n_mels, params.residual_channels, 2**(i % params.dilation_cycle_length), uncond=params.unconditional)\n",
    "        for i in range(params.residual_layers)\n",
    "    ])\n",
    "    self.skip_projection = Conv1d(params.residual_channels, params.residual_channels, 1)\n",
    "    self.output_projection = Conv1d(params.residual_channels, 1, 1)\n",
    "    nn.init.zeros_(self.output_projection.weight)\n",
    "\n",
    "  def forward(self, audio, diffusion_step, spectrogram=None):\n",
    "    assert (spectrogram is None and self.spectrogram_upsampler is None) or \\\n",
    "           (spectrogram is not None and self.spectrogram_upsampler is not None)\n",
    "    x = audio.unsqueeze(1)\n",
    "    x = self.input_projection(x)\n",
    "    x = F.relu(x)\n",
    "\n",
    "    diffusion_step = self.diffusion_embedding(diffusion_step)\n",
    "    if self.spectrogram_upsampler: # use conditional model\n",
    "      spectrogram = self.spectrogram_upsampler(spectrogram)\n",
    "\n",
    "    skip = None\n",
    "    for layer in self.residual_layers:\n",
    "      x, skip_connection = layer(x, diffusion_step, spectrogram)\n",
    "      skip = skip_connection if skip is None else skip_connection + skip\n",
    "\n",
    "    x = skip / sqrt(len(self.residual_layers))\n",
    "    x = self.skip_projection(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.output_projection(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60da82e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#diffwave training body\n",
    "# Copyright 2020 LMNT, Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "from diffwave.dataset import from_path, from_gtzan\n",
    "from diffwave.model import DiffWave\n",
    "from diffwave.params import AttrDict\n",
    "\n",
    "\n",
    "def _nested_map(struct, map_fn):\n",
    "  if isinstance(struct, tuple):\n",
    "    return tuple(_nested_map(x, map_fn) for x in struct)\n",
    "  if isinstance(struct, list):\n",
    "    return [_nested_map(x, map_fn) for x in struct]\n",
    "  if isinstance(struct, dict):\n",
    "    return { k: _nested_map(v, map_fn) for k, v in struct.items() }\n",
    "  return map_fn(struct)\n",
    "\n",
    "\n",
    "class DiffWaveLearner:\n",
    "  def __init__(self, model_dir, model, dataset, optimizer, params, *args, **kwargs):\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    self.model_dir = model_dir\n",
    "    self.model = model\n",
    "    self.dataset = dataset\n",
    "    self.optimizer = optimizer\n",
    "    self.params = params\n",
    "    self.autocast = torch.cuda.amp.autocast(enabled=kwargs.get('fp16', False))\n",
    "    self.scaler = torch.cuda.amp.GradScaler(enabled=kwargs.get('fp16', False))\n",
    "    self.step = 0\n",
    "    self.is_master = True\n",
    "\n",
    "    beta = np.array(self.params.noise_schedule)\n",
    "    noise_level = np.cumprod(1 - beta)\n",
    "    self.noise_level = torch.tensor(noise_level.astype(np.float32))\n",
    "    self.loss_fn = nn.L1Loss()\n",
    "    self.summary_writer = None\n",
    "\n",
    "  def state_dict(self):\n",
    "    if hasattr(self.model, 'module') and isinstance(self.model.module, nn.Module):\n",
    "      model_state = self.model.module.state_dict()\n",
    "    else:\n",
    "      model_state = self.model.state_dict()\n",
    "    return {\n",
    "        'step': self.step,\n",
    "        'model': { k: v.cpu() if isinstance(v, torch.Tensor) else v for k, v in model_state.items() },\n",
    "        'optimizer': { k: v.cpu() if isinstance(v, torch.Tensor) else v for k, v in self.optimizer.state_dict().items() },\n",
    "        'params': dict(self.params),\n",
    "        'scaler': self.scaler.state_dict(),\n",
    "    }\n",
    "\n",
    "  def load_state_dict(self, state_dict):\n",
    "    if hasattr(self.model, 'module') and isinstance(self.model.module, nn.Module):\n",
    "      self.model.module.load_state_dict(state_dict['model'])\n",
    "    else:\n",
    "      self.model.load_state_dict(state_dict['model'])\n",
    "    self.optimizer.load_state_dict(state_dict['optimizer'])\n",
    "    self.scaler.load_state_dict(state_dict['scaler'])\n",
    "    self.step = state_dict['step']\n",
    "\n",
    "  def save_to_checkpoint(self, filename='weights'):\n",
    "    save_basename = f'{filename}-{self.step}.pt'\n",
    "    save_name = f'{self.model_dir}/{save_basename}'\n",
    "    link_name = f'{self.model_dir}/{filename}.pt'\n",
    "    torch.save(self.state_dict(), save_name)\n",
    "    if os.name == 'nt':\n",
    "      torch.save(self.state_dict(), link_name)\n",
    "    else:\n",
    "      if os.path.islink(link_name):\n",
    "        os.unlink(link_name)\n",
    "      os.symlink(save_basename, link_name)\n",
    "\n",
    "  def restore_from_checkpoint(self, filename='weights'):\n",
    "    try:\n",
    "      checkpoint = torch.load(f'{self.model_dir}/{filename}.pt')\n",
    "      self.load_state_dict(checkpoint)\n",
    "      return True\n",
    "    except FileNotFoundError:\n",
    "      return False\n",
    "\n",
    "  def train(self, max_steps=None):\n",
    "    device = next(self.model.parameters()).device\n",
    "    while True:\n",
    "      for features in tqdm(self.dataset, desc=f'Epoch {self.step // len(self.dataset)}') if self.is_master else self.dataset:\n",
    "        if max_steps is not None and self.step >= max_steps:\n",
    "          return\n",
    "        features = _nested_map(features, lambda x: x.to(device) if isinstance(x, torch.Tensor) else x)\n",
    "        loss = self.train_step(features)\n",
    "        if torch.isnan(loss).any():\n",
    "          raise RuntimeError(f'Detected NaN loss at step {self.step}.')\n",
    "        if self.is_master:\n",
    "          if self.step % 50 == 0:\n",
    "            self._write_summary(self.step, features, loss)\n",
    "          if self.step % len(self.dataset) == 0:\n",
    "            self.save_to_checkpoint()\n",
    "        self.step += 1\n",
    "\n",
    "  def train_step(self, features):\n",
    "    for param in self.model.parameters():\n",
    "      param.grad = None\n",
    "\n",
    "    audio = features['audio']\n",
    "    spectrogram = features['spectrogram']\n",
    "\n",
    "    N, T = audio.shape\n",
    "    device = audio.device\n",
    "    self.noise_level = self.noise_level.to(device)\n",
    "\n",
    "    with self.autocast:\n",
    "      t = torch.randint(0, len(self.params.noise_schedule), [N], device=audio.device)\n",
    "      noise_scale = self.noise_level[t].unsqueeze(1)\n",
    "      noise_scale_sqrt = noise_scale**0.5\n",
    "      noise = torch.randn_like(audio)\n",
    "      noisy_audio = noise_scale_sqrt * audio + (1.0 - noise_scale)**0.5 * noise\n",
    "\n",
    "      predicted = self.model(noisy_audio, t, spectrogram)\n",
    "      loss = self.loss_fn(noise, predicted.squeeze(1))\n",
    "\n",
    "    self.scaler.scale(loss).backward()\n",
    "    self.scaler.unscale_(self.optimizer)\n",
    "    self.grad_norm = nn.utils.clip_grad_norm_(self.model.parameters(), self.params.max_grad_norm or 1e9)\n",
    "    self.scaler.step(self.optimizer)\n",
    "    self.scaler.update()\n",
    "    return loss\n",
    "\n",
    "  def _write_summary(self, step, features, loss):\n",
    "    writer = self.summary_writer or SummaryWriter(self.model_dir, purge_step=step)\n",
    "    writer.add_audio('feature/audio', features['audio'][0], step, sample_rate=self.params.sample_rate)\n",
    "    if not self.params.unconditional:\n",
    "      writer.add_image('feature/spectrogram', torch.flip(features['spectrogram'][:1], [1]), step)\n",
    "    writer.add_scalar('train/loss', loss, step)\n",
    "    writer.add_scalar('train/grad_norm', self.grad_norm, step)\n",
    "    writer.flush()\n",
    "    self.summary_writer = writer\n",
    "\n",
    "\n",
    "def _train_impl(replica_id, model, dataset, args, params):\n",
    "  torch.backends.cudnn.benchmark = True\n",
    "  opt = torch.optim.Adam(model.parameters(), lr=params.learning_rate)\n",
    "\n",
    "  learner = DiffWaveLearner(args.model_dir, model, dataset, opt, params, fp16=args.fp16)\n",
    "  learner.is_master = (replica_id == 0)\n",
    "  learner.restore_from_checkpoint()\n",
    "  learner.train(max_steps=args.max_steps)\n",
    "\n",
    "\n",
    "def train(args, params):\n",
    "  if args.data_dirs[0] == 'gtzan':\n",
    "    dataset = from_gtzan(params)\n",
    "  else:\n",
    "    dataset = from_path(args.data_dirs, params)\n",
    "  model = DiffWave(params).cuda()\n",
    "  _train_impl(0, model, dataset, args, params)\n",
    "\n",
    "\n",
    "def train_distributed(replica_id, replica_count, port, args, params):\n",
    "  os.environ['MASTER_ADDR'] = 'localhost'\n",
    "  os.environ['MASTER_PORT'] = str(port)\n",
    "  torch.distributed.init_process_group('nccl', rank=replica_id, world_size=replica_count)\n",
    "  if args.data_dirs[0] == 'gtzan':\n",
    "    dataset = from_gtzan(params, is_distributed=True)\n",
    "  else:\n",
    "    dataset = from_path(args.data_dirs, params, is_distributed=True)\n",
    "  device = torch.device('cuda', replica_id)\n",
    "  torch.cuda.set_device(device)\n",
    "  model = DiffWave(params).to(device)\n",
    "  model = DistributedDataParallel(model, device_ids=[replica_id])\n",
    "  _train_impl(replica_id, model, dataset, args, params)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
